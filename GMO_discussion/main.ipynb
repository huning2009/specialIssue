{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Programmer: Chen Luo, Project Chief: Yang Li\n",
        "\n\n",
        "- Date: 04/09/2019\n",
        "\n",
        "> 要求：\n",
        ">\n",
        ">- 将总体微博语料按照三个时间段进行拆分：09-12；13-15；16-18\n",
        ">\n",
        ">\n",
        ">- 输出结果：词频统计、语义网络（基于词语共现矩阵）\n",
        ">\n",
        ">\n",
        ">- 网络中的节点尽量少一些，思考一个合理的筛选方法"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\nHTML('/Users/lyndon/AnacondaProjects/Style.css')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 67,
          "data": {
            "text/html": [
              "<style>\n",
              "  div.cell{\n",
              "    width: 800px;\n",
              "    margin-left: auto;\n",
              "    margin-right: auto;\n",
              "  }\n",
              "\n",
              "  div.text_cell_render{\n",
              "    line-height: 145%;\n",
              "    width: 800px;\n",
              "    margin-left: auto;\n",
              "    margin-right: auto;\n",
              "  }\n",
              "  .CodeMirror{\n",
              "    font-family: \"Source Code Pro\", source-code-pro, Consolas, monospace;\n",
              "  }\n",
              "\n</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 67,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 时间解析"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T11:45:23.492559Z",
          "start_time": "2019-01-15T11:45:18.802578Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 读入数据\n",
        "gmo1 = pd.read_excel('./GMOData.xlsx', sheet_name='Sheet1', header=0)\n",
        "gmo1.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>_id_x</th>\n",
              "      <th>nick_name</th>\n",
              "      <th>gender</th>\n",
              "      <th>province</th>\n",
              "      <th>city</th>\n",
              "      <th>labrel</th>\n",
              "      <th>birthday</th>\n",
              "      <th>register time</th>\n",
              "      <th>tweets_num</th>\n",
              "      <th>...</th>\n",
              "      <th>person_url</th>\n",
              "      <th>_id_y</th>\n",
              "      <th>user_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>content.1</th>\n",
              "      <th>like_num</th>\n",
              "      <th>repost_num</th>\n",
              "      <th>comment_num</th>\n",
              "      <th>content_cutted</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2.054992e+09</td>\n",
              "      <td>潇潇diana</td>\n",
              "      <td>女</td>\n",
              "      <td>北京</td>\n",
              "      <td>海淀区</td>\n",
              "      <td>NaN</td>\n",
              "      <td>01-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>891.0</td>\n",
              "      <td>...</td>\n",
              "      <td>http://weibo.com/dianahxw</td>\n",
              "      <td>2054992375_Grif48Q52</td>\n",
              "      <td>2.054992e+09</td>\n",
              "      <td>3分钟前</td>\n",
              "      <td>《一个妈妈一天的心路历程》《吃饭篇》牛奶有无三聚氰胺超标，会不会喝成大头？面包有没有添加剂？...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>纯棉 苏丹红 衣服 家里 婴幼儿 琼胶 质地 喇叭 制成 医生 生病 床单 蔬菜 专用 吃饭...</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.773917e+09</td>\n",
              "      <td>睡不饱的任镳</td>\n",
              "      <td>男</td>\n",
              "      <td>上海</td>\n",
              "      <td>杨浦区</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1986-05-21</td>\n",
              "      <td>40389.0</td>\n",
              "      <td>1353.0</td>\n",
              "      <td>...</td>\n",
              "      <td>http://weibo.com/u/1773916610</td>\n",
              "      <td>1773916610_GrieZ88DV</td>\n",
              "      <td>1.773917e+09</td>\n",
              "      <td>3分钟前</td>\n",
              "      <td>我发现现在的媒体，有了微博后，关注度会大幅度增加，快速传播，影响很大，不过有个缺点，就是不够...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>天津 国内 顶尖 喜好 缺点 添上 崔 不伦不类 专业性 花荣 有名 股侠 传播 缺陷 快速...</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.773917e+09</td>\n",
              "      <td>睡不饱的任镳</td>\n",
              "      <td>男</td>\n",
              "      <td>上海</td>\n",
              "      <td>杨浦区</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1986-05-21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1353.0</td>\n",
              "      <td>...</td>\n",
              "      <td>http://weibo.com/u/1773916610</td>\n",
              "      <td>1773916610_Acvjj1CiK</td>\n",
              "      <td>1.773917e+09</td>\n",
              "      <td>2013-10-04 22:12:09</td>\n",
              "      <td>院士：中国没有拒绝转基因技术的资本】据工程院院士吴孔明分析，我国粮食产需间矛盾突出，已经不允...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>详见 发展 粮食 作物 zR2wIgX 危险 吴 棉花 搁置 院士 资本 放弃 技术 拒绝 ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3.021199e+09</td>\n",
              "      <td>狙击手蝈蝈</td>\n",
              "      <td>男</td>\n",
              "      <td>广东</td>\n",
              "      <td>东莞</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>...</td>\n",
              "      <td>http://weibo.com/u/3021199493</td>\n",
              "      <td>3021199493_Gribg5cCN</td>\n",
              "      <td>3.021199e+09</td>\n",
              "      <td>13分钟前</td>\n",
              "      <td>铁证如山！日军性暴行受害者两姐妹证言公布】为救其他人，时年14岁的彭仁寿被强征为“慰安妇”，...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>强征 救 日军 彭 传递 彭竹英 刀疤 触目 慰安妇 终生 细菌武器 魔爪 未能 逃脱 受害...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3.021199e+09</td>\n",
              "      <td>狙击手蝈蝈</td>\n",
              "      <td>男</td>\n",
              "      <td>广东</td>\n",
              "      <td>东莞</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>...</td>\n",
              "      <td>http://weibo.com/u/3021199493</td>\n",
              "      <td>3021199493_EmYJuh1za</td>\n",
              "      <td>3.021199e+09</td>\n",
              "      <td>2016-12-20 14:26:32</td>\n",
              "      <td>黑龙江省明年5月起全面禁止转基因作物！】根据最新修订的《黑龙江省食品安全条例》，自2017年...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>含有 RIcPEQm 销售 作物 粮食产量 食品安全 条例 大省 加工 大豆 全面禁止 明年...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0         _id_x nick_name gender province city labrel    birthday  \\\n",
              "0           0  2.054992e+09   潇潇diana      女       北京  海淀区    NaN       01-01   \n",
              "1           1  1.773917e+09    睡不饱的任镳      男       上海  杨浦区    NaN  1986-05-21   \n",
              "2           2  1.773917e+09    睡不饱的任镳      男       上海  杨浦区    NaN  1986-05-21   \n",
              "3           3  3.021199e+09     狙击手蝈蝈      男       广东   东莞    NaN         NaN   \n",
              "4           4  3.021199e+09     狙击手蝈蝈      男       广东   东莞    NaN         NaN   \n",
              "\n",
              "   register time  tweets_num  ...                     person_url  \\\n",
              "0            NaN       891.0  ...      http://weibo.com/dianahxw   \n",
              "1        40389.0      1353.0  ...  http://weibo.com/u/1773916610   \n",
              "2            NaN      1353.0  ...  http://weibo.com/u/1773916610   \n",
              "3            NaN      3329.0  ...  http://weibo.com/u/3021199493   \n",
              "4            NaN      3329.0  ...  http://weibo.com/u/3021199493   \n",
              "\n",
              "                  _id_y       user_id           created_at  \\\n",
              "0  2054992375_Grif48Q52  2.054992e+09                 3分钟前   \n",
              "1  1773916610_GrieZ88DV  1.773917e+09                 3分钟前   \n",
              "2  1773916610_Acvjj1CiK  1.773917e+09  2013-10-04 22:12:09   \n",
              "3  3021199493_Gribg5cCN  3.021199e+09                13分钟前   \n",
              "4  3021199493_EmYJuh1za  3.021199e+09  2016-12-20 14:26:32   \n",
              "\n",
              "                                           content.1 like_num repost_num  \\\n",
              "0  《一个妈妈一天的心路历程》《吃饭篇》牛奶有无三聚氰胺超标，会不会喝成大头？面包有没有添加剂？...      0.0        0.0   \n",
              "1  我发现现在的媒体，有了微博后，关注度会大幅度增加，快速传播，影响很大，不过有个缺点，就是不够...      0.0        0.0   \n",
              "2  院士：中国没有拒绝转基因技术的资本】据工程院院士吴孔明分析，我国粮食产需间矛盾突出，已经不允...      0.0        0.0   \n",
              "3  铁证如山！日军性暴行受害者两姐妹证言公布】为救其他人，时年14岁的彭仁寿被强征为“慰安妇”，...      0.0        0.0   \n",
              "4  黑龙江省明年5月起全面禁止转基因作物！】根据最新修订的《黑龙江省食品安全条例》，自2017年...      0.0        1.0   \n",
              "\n",
              "   comment_num                                     content_cutted topic  \n",
              "0          0.0  纯棉 苏丹红 衣服 家里 婴幼儿 琼胶 质地 喇叭 制成 医生 生病 床单 蔬菜 专用 吃饭...    12  \n",
              "1          0.0  天津 国内 顶尖 喜好 缺点 添上 崔 不伦不类 专业性 花荣 有名 股侠 传播 缺陷 快速...    19  \n",
              "2          0.0  详见 发展 粮食 作物 zR2wIgX 危险 吴 棉花 搁置 院士 资本 放弃 技术 拒绝 ...     5  \n",
              "3          1.0  强征 救 日军 彭 传递 彭竹英 刀疤 触目 慰安妇 终生 细菌武器 魔爪 未能 逃脱 受害...     8  \n",
              "4          1.0  含有 RIcPEQm 销售 作物 粮食产量 食品安全 条例 大省 加工 大豆 全面禁止 明年...     3  \n",
              "\n[5 rows x 25 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T10:26:54.202771Z",
          "start_time": "2019-01-15T10:24:32.751598Z"
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据行列数\n",
        "print('%s 行 %s 列'%(gmo1.shape[0], gmo1.shape[1]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "886837 行 25 列\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T10:28:21.347052Z",
          "start_time": "2019-01-15T10:28:21.343954Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 抽取内容列、日期列\n",
        "gmo2 = gmo1[['content.1', 'created_at']]\n",
        "gmo2.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": [
              "(886837, 2)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T10:31:11.368113Z",
          "start_time": "2019-01-15T10:31:11.317330Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据备份\n",
        "gmo2.to_excel('./v2/gmo.xlsx', index=None)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T10:38:59.723714Z",
          "start_time": "2019-01-15T10:37:55.284331Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据复制\n",
        "gmo3 = gmo2.copy()\n",
        "\n",
        "# 抽取年份\n",
        "pattern_match = re.compile('(\\d{4}-\\d{2}-\\d{2})')\n",
        "def getYear(s):\n",
        "    yearMatch = pattern_match.search(str(s))\n",
        "    if yearMatch != None:\n",
        "        year = yearMatch.group(1).split('-')[0].strip()\n",
        "    else:\n",
        "        year = '2018'\n",
        "    return year\n",
        "\n",
        "gmo3['year'] = gmo3['created_at'].apply(getYear)\n",
        "gmo3.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content.1</th>\n",
              "      <th>created_at</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>《一个妈妈一天的心路历程》《吃饭篇》牛奶有无三聚氰胺超标，会不会喝成大头？面包有没有添加剂？...</td>\n",
              "      <td>3分钟前</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我发现现在的媒体，有了微博后，关注度会大幅度增加，快速传播，影响很大，不过有个缺点，就是不够...</td>\n",
              "      <td>3分钟前</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>院士：中国没有拒绝转基因技术的资本】据工程院院士吴孔明分析，我国粮食产需间矛盾突出，已经不允...</td>\n",
              "      <td>2013-10-04 22:12:09</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>铁证如山！日军性暴行受害者两姐妹证言公布】为救其他人，时年14岁的彭仁寿被强征为“慰安妇”，...</td>\n",
              "      <td>13分钟前</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>黑龙江省明年5月起全面禁止转基因作物！】根据最新修订的《黑龙江省食品安全条例》，自2017年...</td>\n",
              "      <td>2016-12-20 14:26:32</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           content.1           created_at  \\\n",
              "0  《一个妈妈一天的心路历程》《吃饭篇》牛奶有无三聚氰胺超标，会不会喝成大头？面包有没有添加剂？...                 3分钟前   \n",
              "1  我发现现在的媒体，有了微博后，关注度会大幅度增加，快速传播，影响很大，不过有个缺点，就是不够...                 3分钟前   \n",
              "2  院士：中国没有拒绝转基因技术的资本】据工程院院士吴孔明分析，我国粮食产需间矛盾突出，已经不允...  2013-10-04 22:12:09   \n",
              "3  铁证如山！日军性暴行受害者两姐妹证言公布】为救其他人，时年14岁的彭仁寿被强征为“慰安妇”，...                13分钟前   \n",
              "4  黑龙江省明年5月起全面禁止转基因作物！】根据最新修订的《黑龙江省食品安全条例》，自2017年...  2016-12-20 14:26:32   \n",
              "\n",
              "   year  \n",
              "0  2018  \n",
              "1  2018  \n",
              "2  2013  \n",
              "3  2018  \n",
              "4  2016  "
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T10:52:19.046348Z",
          "start_time": "2019-01-15T10:52:17.942295Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 微博数量按年份统计\n",
        "groupYear = gmo3.groupby('year')\n",
        "groupYear.size()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": [
              "year\n",
              "2009       145\n",
              "2010     10997\n",
              "2011     47428\n",
              "2012    115129\n",
              "2013    142112\n",
              "2014    140611\n",
              "2015    106531\n",
              "2016    132339\n",
              "2017    121113\n",
              "2018     70432\n",
              "dtype: int64"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T10:55:13.647844Z",
          "start_time": "2019-01-15T10:55:13.453851Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 微博总数量\n",
        "print(len(gmo3['year']))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "886837\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 抽样"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 09-12年微博总量：173699\n",
        "gmo_09_12 = gmo3[gmo3['year'].isin(['2009', '2010', '2011', '2012'])]\n",
        "print(gmo_09_12.shape)\n",
        "\n",
        "sample_09_12 = gmo_09_12.sample(n=20000, random_state=2009)\n",
        "print(sample_09_12.shape)\n",
        "\nsample_09_12.to_excel('./v2/09_12.xlsx', index=None)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(173699, 3)\n",
            "(20000, 3)\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T11:52:27.530062Z",
          "start_time": "2019-01-15T11:52:24.131901Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13-15年微博总量：389254\n",
        "gmo_13_15 = gmo3[gmo3['year'].isin(['2013', '2014', '2015'])]\n",
        "print(gmo_13_15.shape)\n",
        "\n",
        "sample_13_15 = gmo_13_15.sample(n=20000, random_state=2013)\n",
        "print(sample_13_15.shape)\n",
        "\nsample_13_15.to_excel('./v2/13_15.xlsx', index=None)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(389254, 3)\n",
            "(20000, 3)\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T11:54:36.203847Z",
          "start_time": "2019-01-15T11:54:33.765819Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16-18年微博总量：323884\n",
        "gmo_16_18 = gmo3[gmo3['year'].isin(['2016', '2017', '2018'])]\n",
        "print(gmo_16_18.shape)\n",
        "\n",
        "sample_16_18 = gmo_16_18.sample(n=20000, random_state=2016)\n",
        "print(sample_16_18.shape)\n",
        "\nsample_16_18.to_excel('./v2/16_18.xlsx', index=None)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(323884, 3)\n",
            "(20000, 3)\n"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-15T11:55:50.563349Z",
          "start_time": "2019-01-15T11:55:48.217683Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分词、统计词频"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import codecs\n",
        "import jieba\n",
        "import jieba.posseg as pseg\n",
        "from multiprocessing import Pool\n",
        "from collections import defaultdict"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取自定义词典\n",
        "jieba.load_userdict('./v2/userdict.txt')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /var/folders/_6/73bms9_11733g1hbd2_jmrzc0000gn/T/jieba.cache\n",
            "Loading model cost 0.778 seconds.\n",
            "Prefix dict has been built succesfully.\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入停用词\n",
        "stopwords = list()\n",
        "with open('./v2/stopwords.txt', 'r') as s:\n",
        "    for word in s.readlines():\n",
        "        stopwords.append(word.strip())\n",
        "stopwords = list(set(stopwords))\n",
        "print(len(stopwords))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2297\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 每次返回2000条数据\n",
        "def loadData(df):\n",
        "    trunk = 2000\n",
        "    idx = 0\n",
        "    contents = list()\n",
        "    for line in df['content.1']:\n",
        "        contents.append(str(line).strip())\n",
        "        idx += 1\n",
        "        if idx % trunk == 0:\n",
        "            yield contents\n",
        "            contents = list()"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 分词函数，仅保留名词、形容词、动词\n",
        "# 词性参考 https://blog.csdn.net/suibianshen2012/article/details/53487157\n",
        "def tokenize(contents):\n",
        "    result = list()\n",
        "    for content in contents:\n",
        "        _ = list()\n",
        "        for word, flag in pseg.cut(content):\n",
        "            if (word not in stopwords) and (len(word) > 1) and (re.search(r'^n|a|v', flag) != None):\n",
        "                _.append(word.strip())\n",
        "        result.append(' '.join(_))\n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 并行分词\n",
        "def parTokenize(file, df):\n",
        "    target = codecs.open('./v2/' + file.split('/')[-1].split('.')[0] + '_seg.txt', 'w', 'utf-8')\n",
        "    contents = loadData(df)\n",
        "    cpus = 5\n",
        "    ichunk = 0\n",
        "    for content in contents:\n",
        "        pool = Pool(cpus)\n",
        "        step = int(len(content) / cpus)\n",
        "        temp = [content[i:i+step] for i in range(0, len(content), step)]\n",
        "        results = pool.map(tokenize, temp)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        for r in results:\n",
        "            for i in r:\n",
        "                target.write(i + '\\n')\n",
        "        ichunk += 1\n",
        "        print('Progress:', len(content)*ichunk)\n",
        "    target.close()"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 执行并行分词\n",
        "meta_path = './v2/'\n",
        "for file in os.listdir('./v2'):\n",
        "    if re.search(r'(\\d{2}_\\d{2}.xlsx)', file):\n",
        "        parTokenize(meta_path + file, pd.read_excel(meta_path + file, header=0))\n",
        "        print(file, 'finished.', '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 2000\n",
            "Progress: 4000\n",
            "Progress: 6000\n",
            "Progress: 8000\n",
            "Progress: 10000\n",
            "Progress: 12000\n",
            "Progress: 14000\n",
            "Progress: 16000\n",
            "Progress: 18000\n",
            "Progress: 20000\n",
            "09_12.xlsx finished. \n",
            "\n",
            ".DS_Store finished. \n",
            "\n",
            "Progress: 2000\n",
            "Progress: 4000\n",
            "Progress: 6000\n",
            "Progress: 8000\n",
            "Progress: 10000\n",
            "Progress: 12000\n",
            "Progress: 14000\n",
            "Progress: 16000\n",
            "Progress: 18000\n",
            "Progress: 20000\n",
            "13_15.xlsx finished. \n",
            "\n",
            "userdict.txt finished. \n",
            "\n",
            "stopwords.txt finished. \n",
            "\n",
            "gmo.xlsx finished. \n",
            "\n",
            "Progress: 2000\n",
            "Progress: 4000\n",
            "Progress: 6000\n",
            "Progress: 8000\n",
            "Progress: 10000\n",
            "Progress: 12000\n",
            "Progress: 14000\n",
            "Progress: 16000\n",
            "Progress: 18000\n",
            "Progress: 20000\n",
            "16_18.xlsx finished. \n",
            "\n"
          ]
        }
      ],
      "execution_count": 46,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 统计词频\n",
        "def freq(file):\n",
        "    frequency = defaultdict(int)\n",
        "    target = codecs.open('./v2/' + file.split('/')[-1].split('.')[0].replace('seg', 'freq') + '.txt', 'w', 'utf-8')\n",
        "    source = codecs.open(file, 'r', 'utf-8')\n",
        "    for line in source.readlines():\n",
        "        for word in line.split():\n",
        "            frequency[word] += 1\n",
        "    for w, f in sorted(dict(frequency).items(), key=lambda x: x[1], reverse=True):\n",
        "        target.write(w + ' ' + str(f) + '\\n')\n",
        "\n",
        "for file in os.listdir('./v2/'):\n",
        "    if re.search(r'\\d{2}_\\d{2}_seg.txt', file):\n",
        "        freq('./v2/' + file)"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 构建网络\n",
        "- 需要排除搜索词“转基因”（dominant word），避免对后续网络分析造成干扰"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 每一行的分词需要去重，再统计共现关系\n",
        "def co(file):\n",
        "    co_occur = defaultdict(int)\n",
        "    # 排除“转基因”的文件名中包含refine\n",
        "    target = codecs.open('./v2/' + file.split('/')[-1].split('.')[0].replace('seg', 'network_refine') + '.csv', 'w', 'utf-8')\n",
        "    source = codecs.open(file, 'r', 'utf-8')\n",
        "    for line in source.readlines():\n",
        "        new_line = list(set(line.split()))\n",
        "        new_line.sort()\n",
        "        for pair in combinations(new_line, 2):\n",
        "            if '转基因' not in pair:\n",
        "                co_occur[pair] += 1\n",
        "    for p, f in sorted(dict(co_occur).items(), key=lambda x: x[1], reverse=True):\n",
        "        target.write(p[0] + ',' + p[1] + ',' + str(f) + '\\n')\n",
        "\n",
        "for file in os.listdir('./v2/'):\n",
        "    if re.search(r'\\d{2}_\\d{2}_seg.txt', file):\n",
        "        co('./v2/' + file)"
      ],
      "outputs": [],
      "execution_count": 72,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# try\n",
        "l = [12, 23, 34]\n",
        "for i in combinations(l, 2):\n",
        "    print(i, i[0], type(i), 12 in i)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12, 23) 12 <class 'tuple'> True\n",
            "(12, 34) 12 <class 'tuple'> True\n",
            "(23, 34) 23 <class 'tuple'> False\n"
          ]
        }
      ],
      "execution_count": 71,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 选择呈现节点\n",
        "- 先按照频数从高到低排序，然后设定比率"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for file in os.listdir('./v2/'):\n",
        "    # 用 *_refine.csv 文件\n",
        "    if re.search(r'\\d{2}_\\d{2}_network_refine.csv', file):\n",
        "        df = pd.read_csv('./v2/' + file, sep=',', header=None)\n",
        "        frequency = list(set(list(df[2])))\n",
        "        frequency.sort(reverse=True)\n",
        "        print(frequency, '\\n')\n",
        "        print('文件名：', file, '\\n', '频次单独值数量：', len(frequency), '\\n', '截尾值：', frequency[round(len(frequency) * 0.25) + 1], '\\n\\n\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1082, 556, 555, 540, 477, 439, 416, 380, 379, 365, 360, 346, 323, 320, 315, 309, 308, 304, 299, 281, 280, 274, 272, 271, 270, 261, 260, 258, 256, 248, 247, 245, 242, 240, 233, 229, 226, 224, 223, 222, 220, 216, 214, 205, 204, 202, 201, 200, 199, 195, 189, 188, 186, 183, 181, 180, 179, 178, 177, 176, 175, 174, 173, 170, 169, 166, 165, 164, 160, 159, 158, 157, 156, 155, 153, 152, 151, 149, 147, 145, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 110, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] \n",
            "\n",
            "文件名： 13_15_network_refine.csv \n",
            " 频次单独值数量： 221 \n",
            " 截尾值： 179 \n",
            "\n\n\n",
            "[1115, 699, 537, 489, 416, 409, 393, 387, 381, 379, 364, 360, 352, 348, 344, 342, 334, 327, 326, 321, 317, 306, 298, 297, 296, 277, 264, 261, 260, 256, 253, 249, 248, 247, 243, 241, 238, 237, 236, 235, 223, 222, 221, 220, 216, 215, 214, 213, 212, 211, 210, 208, 206, 205, 203, 199, 198, 195, 194, 193, 192, 191, 188, 187, 186, 183, 182, 181, 180, 179, 177, 176, 175, 174, 173, 172, 168, 167, 165, 163, 161, 158, 157, 155, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] \n",
            "\n",
            "文件名： 09_12_network_refine.csv \n",
            " 频次单独值数量： 237 \n",
            " 截尾值： 192 \n",
            "\n\n\n",
            "[1034, 770, 660, 659, 658, 656, 627, 547, 508, 506, 469, 437, 429, 424, 383, 377, 374, 373, 344, 329, 326, 320, 316, 312, 307, 296, 283, 282, 276, 274, 273, 265, 262, 261, 256, 255, 254, 248, 246, 244, 242, 230, 229, 227, 222, 219, 217, 215, 212, 211, 206, 202, 200, 199, 198, 197, 193, 192, 189, 186, 184, 182, 181, 180, 179, 178, 177, 176, 175, 173, 172, 169, 163, 162, 160, 158, 157, 156, 154, 153, 152, 151, 149, 148, 147, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] \n",
            "\n",
            "文件名： 16_18_network_refine.csv \n",
            " 频次单独值数量： 229 \n",
            " 截尾值： 189 \n",
            "\n\n\n"
          ]
        }
      ],
      "execution_count": 74,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gephi绘图\n",
        "- 节点重要性：特征向量中心度\n",
        "\n\n",
        "- 模块化处理（目的：社区侦测；注意计算细节，如：使用权重），不同颜色渲染节点\n",
        "\n\n- 语义网络核心文献"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}